{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from SoundLights.features_groups import  ARAUS_features, Freesound_features, mix_features, masker_features\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "\n",
    "# Create a RobustScaler object\n",
    "scaler = RobustScaler()\n",
    "\n",
    "normalise = lambda X: (X-np.mean(X,axis=0,keepdims=True))/np.std(X,axis=0,keepdims=True) # Normalise an (n,p) numpy array to mean 0, variance 1.\n",
    "clip = lambda x, x_min = -1, x_max = 1: np.where(np.where(x < x_min,x_min,x) > x_max, x_max, np.where(x < x_min,x_min,x)) # Clip an array to values between x_min and x_max.\n",
    "def normalize_columns(data):\n",
    "    # Calculate mean and standard deviation for each column\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "    \n",
    "    # Normalize each column separately\n",
    "    normalized_data = (data - mean) / (std)\n",
    "    \n",
    "    return normalized_data, mean, std\n",
    "\n",
    "def normalize_columns_minmax(data):\n",
    "    # Calculate mean and standard deviation for each column\n",
    "    min = np.min(data, axis=0)\n",
    "    max = np.max(data, axis=0)\n",
    "    \n",
    "    # Normalize each column separately\n",
    "    normalized_data = (data - min) / (max-min)\n",
    "    \n",
    "    return normalized_data, min, max\n",
    "\n",
    "def normalize_columns_log(data):\n",
    "    log_transformed_data = np.log(data + 1)\n",
    "\n",
    "    return log_transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_Freesound= pd.read_csv(os.path.join('..','data/csv_files','SoundLights_Freesound.csv')) #, dtype = {'participant':str}\n",
    "responses_Freesound=responses_Freesound.drop(\"info.file\", axis=1)\n",
    "responses_Freesound=responses_Freesound.drop(\"info.participant\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_Mix= pd.read_csv(os.path.join('..','data/csv_files','SoundLights_mix.csv')) #, dtype = {'participant':str}\n",
    "responses_Mix=responses_Mix.drop(\"info.file\", axis=1)\n",
    "responses_Mix=responses_Mix.drop(\"info.participant\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_complete= pd.read_csv(os.path.join('..','data/csv_files','SoundLights_complete.csv')) #, dtype = {'participant':str}\n",
    "responses_complete=responses_complete.drop(\"info.file\", axis=1)\n",
    "responses_complete=responses_complete.drop(\"info.participant\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ARAUS.sharpness.avg', 'ARAUS.sharpness.max', 'ARAUS.sharpness.p05', 'ARAUS.sharpness.p10', 'ARAUS.sharpness.p20', 'ARAUS.sharpness.p30', 'ARAUS.sharpness.p40', 'ARAUS.sharpness.p50', 'ARAUS.sharpness.p60', 'ARAUS.sharpness.p70', 'ARAUS.sharpness.p80', 'ARAUS.sharpness.p90', 'ARAUS.sharpness.p95', 'ARAUS.loudness.avg', 'ARAUS.loudness.max', 'ARAUS.loudness.p05', 'ARAUS.loudness.p10', 'ARAUS.loudness.p20', 'ARAUS.loudness.p30', 'ARAUS.loudness.p40', 'ARAUS.loudness.p50', 'ARAUS.loudness.p60', 'ARAUS.loudness.p70', 'ARAUS.loudness.p80', 'ARAUS.loudness.p90', 'ARAUS.loudness.p95', 'ARAUS.fluctuation.avg', 'ARAUS.fluctuation.max', 'ARAUS.fluctuation.p05', 'ARAUS.fluctuation.p10', 'ARAUS.fluctuation.p20', 'ARAUS.fluctuation.p30', 'ARAUS.fluctuation.p40', 'ARAUS.fluctuation.p50', 'ARAUS.fluctuation.p60', 'ARAUS.fluctuation.p70', 'ARAUS.fluctuation.p80', 'ARAUS.fluctuation.p90', 'ARAUS.fluctuation.p95', 'ARAUS.LA.avg', 'ARAUS.LA.min', 'ARAUS.LA.max', 'ARAUS.LA.p05', 'ARAUS.LA.p10', 'ARAUS.LA.p20', 'ARAUS.LA.p30', 'ARAUS.LA.p40', 'ARAUS.LA.p50', 'ARAUS.LA.p60', 'ARAUS.LA.p70', 'ARAUS.LA.p80', 'ARAUS.LA.p90', 'ARAUS.LA.p95', 'ARAUS.LC.avg', 'ARAUS.LC.min', 'ARAUS.LC.max', 'ARAUS.LC.p05', 'ARAUS.LC.p10', 'ARAUS.LC.p20', 'ARAUS.LC.p30', 'ARAUS.LC.p40', 'ARAUS.LC.p50', 'ARAUS.LC.p60', 'ARAUS.LC.p70', 'ARAUS.LC.p80', 'ARAUS.LC.p90', 'ARAUS.LC.p95', 'ARAUS.roughness.avg', 'ARAUS.roughness.max', 'ARAUS.roughness.p05', 'ARAUS.roughness.p10', 'ARAUS.roughness.p20', 'ARAUS.roughness.p30', 'ARAUS.roughness.p40', 'ARAUS.roughness.p50', 'ARAUS.roughness.p60', 'ARAUS.roughness.p70', 'ARAUS.roughness.p80', 'ARAUS.roughness.p90', 'ARAUS.roughness.p95', 'ARAUS.energy_frequency.00006_3', 'ARAUS.energy_frequency.00012_5', 'ARAUS.energy_frequency.00016_0', 'ARAUS.energy_frequency.00025_0', 'ARAUS.energy_frequency.00031_5', 'ARAUS.energy_frequency.00040_0', 'ARAUS.energy_frequency.00050_0', 'ARAUS.energy_frequency.00063_0', 'ARAUS.energy_frequency.00080_0', 'ARAUS.energy_frequency.00100_0', 'ARAUS.energy_frequency.00125_0', 'ARAUS.energy_frequency.00160_0', 'ARAUS.energy_frequency.00200_0', 'ARAUS.energy_frequency.00250_0', 'ARAUS.energy_frequency.00315_0', 'ARAUS.energy_frequency.00400_0', 'ARAUS.energy_frequency.00500_0', 'ARAUS.energy_frequency.00630_0', 'ARAUS.energy_frequency.00800_0', 'ARAUS.energy_frequency.01000_0', 'ARAUS.energy_frequency.01250_0', 'ARAUS.energy_frequency.01600_0', 'ARAUS.energy_frequency.02000_0', 'ARAUS.energy_frequency.02500_0', 'ARAUS.energy_frequency.03150_0', 'ARAUS.energy_frequency.04000_0', 'ARAUS.energy_frequency.05000_0', 'ARAUS.energy_frequency.06300_0', 'ARAUS.energy_frequency.08000_0', 'ARAUS.energy_frequency.10000_0', 'ARAUS.energy_frequency.12500_0', 'ARAUS.energy_frequency.16000_0', 'ARAUS.energy_frequency.20000_0']\n"
     ]
    }
   ],
   "source": [
    "responses_ARAUS= pd.read_csv(os.path.join('..','data/csv_files','SoundLights_ARAUS.csv'), dtype = {'info.participant':str}) #, dtype = {'participant':str}\n",
    "responses_ARAUS=responses_ARAUS.drop(\"info.file\", axis=1)\n",
    "responses_ARAUS=responses_ARAUS.drop(\"info.participant\", axis=1)\n",
    "\n",
    "# Drop columns that contain all zero values\n",
    "# Store column names before dropping\n",
    "columns_before = responses_ARAUS.columns.tolist()\n",
    "# Drop zero-columns\n",
    "responses_ARAUS = responses_ARAUS.loc[:, (responses_ARAUS != 0).any(axis=0)]\n",
    "# Store column names after dropping\n",
    "columns_after = responses_ARAUS.columns.tolist()\n",
    "# Determine which columns were dropped\n",
    "columns_dropped = [col for col in columns_before if col not in columns_after]\n",
    "# Drop those columns from ARAUS_features\n",
    "ARAUS_features = [col for col in ARAUS_features if col not in columns_dropped]\n",
    "print(ARAUS_features)\n",
    "#MAskers colum, increase values\n",
    "responses_ARAUS[\"info.masker_bird\"]=responses_ARAUS[\"info.masker_bird\"]*5\n",
    "responses_ARAUS[\"info.masker_construction\"]=responses_ARAUS[\"info.masker_construction\"]*5\n",
    "responses_ARAUS[\"info.masker_traffic\"]=responses_ARAUS[\"info.masker_traffic\"]*5\n",
    "responses_ARAUS[\"info.masker_water\"]=responses_ARAUS[\"info.masker_water\"]*5\n",
    "responses_ARAUS[\"info.masker_wind\"]=responses_ARAUS[\"info.masker_wind\"]*5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |    Mean squared error    |        Mean  error       |         |       # samples      | #     | # NZ \n",
      "Fold |--------+--------+--------|--------+--------+--------| Inter-  |-------+-------+------| feat- | feat-\n",
      "     | Train  |   Val  |  Test  | Train  |   Val  |  Test  |  cept   | Train |  Val  | Test | ures  | ures \n",
      "-----+--------+--------+--------+--------+--------+--------+---------+-------+-------+------+-------+------\n",
      "Parameters  0.1 0.5\n",
      "Mean | 0.1292 | 0.1323 | 0.0353 | 0.2960 | 0.2994 | 0.1493 |\n",
      "Parameters  0.2 0.5\n",
      "Mean | 0.1308 | 0.1329 | 0.0323 | 0.2983 | 0.3006 | 0.1433 |\n",
      "Parameters  0.3 0.5\n",
      "Mean | 0.1315 | 0.1333 | 0.0317 | 0.2996 | 0.3015 | 0.1441 |\n",
      "Parameters  0.4 0.5\n",
      "Mean | 0.1321 | 0.1337 | 0.0322 | 0.3007 | 0.3022 | 0.1472 |\n",
      "Parameters  0.5 0.5\n",
      "Mean | 0.1328 | 0.1342 | 0.0329 | 0.3018 | 0.3032 | 0.1510 |\n",
      "Parameters  0.6 0.5\n",
      "Mean | 0.1336 | 0.1348 | 0.0341 | 0.3030 | 0.3043 | 0.1552 |\n",
      "Parameters  0.7 0.5\n",
      "Mean | 0.1345 | 0.1356 | 0.0356 | 0.3044 | 0.3055 | 0.1596 |\n",
      "Parameters  0.8 0.5\n",
      "Mean | 0.1356 | 0.1365 | 0.0375 | 0.3059 | 0.3068 | 0.1642 |\n",
      "Parameters  0.9 0.5\n",
      "Mean | 0.1366 | 0.1373 | 0.0394 | 0.3072 | 0.3079 | 0.1689 |\n",
      "Parameters  1 0.5\n",
      "Mean | 0.1373 | 0.1378 | 0.0411 | 0.3082 | 0.3087 | 0.1730 |\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "# Suppress ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "# Define your ElasticNet model with specific hyperparameters\n",
    "alpha = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1] \n",
    "l1_ratio = 0.5\n",
    "print('     |    Mean squared error    |        Mean  error       |         |       # samples      | #     | # NZ ')\n",
    "print('Fold |--------+--------+--------|--------+--------+--------| Inter-  |-------+-------+------| feat- | feat-')\n",
    "print('     | Train  |   Val  |  Test  | Train  |   Val  |  Test  |  cept   | Train |  Val  | Test | ures  | ures ')\n",
    "print('-----+--------+--------+--------+--------+--------+--------+---------+-------+-------+------+-------+------')\n",
    "\n",
    "for value in alpha:\n",
    "\n",
    "    model = ElasticNet(alpha=value, l1_ratio=l1_ratio, selection=\"random\")\n",
    "    #print(f'Investigating performance of {model} model...')\n",
    "\n",
    "    MSEs_train = []\n",
    "    MSEs_val = []\n",
    "    MSEs_test = []\n",
    "    MEs_train = []\n",
    "    MEs_val = []\n",
    "    MEs_test = []\n",
    "\n",
    "    \n",
    "    for val_fold in [1,2,3,4,5]:\n",
    "\n",
    "        # Extract dataframes\n",
    "        df_train = responses_ARAUS[(responses_ARAUS['info.fold'] != val_fold) & (responses_ARAUS['info.fold'] > 0)] # For the training set, use all samples that are not in the test set (fold 0) and current validation fold.\n",
    "        df_val   = responses_ARAUS[responses_ARAUS['info.fold'] == val_fold]\n",
    "        df_test  = responses_ARAUS[responses_ARAUS['info.fold'] == 0].groupby(['info.soundscape','info.masker','info.smr']).mean() # For the test set, the same 48 stimuli were shown to all participants so we take the mean of their ratings as the ground truth\n",
    "\n",
    "        # Get ground-truth labels\n",
    "        Y_train = df_train['info.E_ground_truth'].values\n",
    "        Y_val = df_val['info.E_ground_truth'].values\n",
    "        Y_test = df_test['info.E_ground_truth'].values\n",
    "\n",
    "\n",
    "        # Get features normalized_data = (data - min) / (max-min)\n",
    "        X_train = df_train[ARAUS_features+masker_features].values\n",
    "        X_val =df_val[ARAUS_features+masker_features].values\n",
    "        X_test = df_test[ARAUS_features+masker_features].values\n",
    "        \"\"\" X_train, mean, std = normalize_columns(df_train[ARAUS_features+masker_features].values)\n",
    "        X_val =((df_val[ARAUS_features+masker_features].values)- mean) / (std)\n",
    "        X_test = ((df_test[ARAUS_features+masker_features].values)- mean) / (std)  \"\"\"\n",
    "        \n",
    "        \n",
    "        \"\"\" scaler.fit(df_train[ARAUS_features].values)\n",
    "\n",
    "        # Transform the data using the scaler\n",
    "        X_train = scaler.transform(df_train[ARAUS_features].values)\n",
    "        X_val =scaler.transform(df_val[ARAUS_features].values)\n",
    "        X_test = scaler.transform(df_test[ARAUS_features].values) \"\"\"\n",
    "\n",
    "        # Fit model\n",
    "        X_LR = model.fit(X_train, Y_train)\n",
    "\n",
    "        # Get MSEs\n",
    "        MSE_train = np.mean((clip(X_LR.predict(X_train)) - Y_train)**2)\n",
    "        MSE_val = np.mean((clip(X_LR.predict(X_val)) - Y_val)**2)\n",
    "        MSE_test = np.mean((clip(X_LR.predict(X_test)) - Y_test)**2)\n",
    "        ME_train = np.mean(np.abs(clip(X_LR.predict(X_train)) - Y_train))\n",
    "        ME_val = np.mean(np.abs(clip(X_LR.predict(X_val)) - Y_val))\n",
    "        ME_test = np.mean(np.abs(clip(X_LR.predict(X_test)) - Y_test))\n",
    "\n",
    "        # Add metrics\n",
    "        MSEs_train.append(MSE_train)\n",
    "        MSEs_val.append(MSE_val)\n",
    "        MSEs_test.append(MSE_test)\n",
    "        MEs_train.append(ME_train)\n",
    "        MEs_val.append(ME_val)\n",
    "        MEs_test.append(ME_test)\n",
    "\n",
    "        #print(f'{val_fold:4d} | {MSE_train:.4f} | {MSE_val:.4f} | {MSE_test:.4f} | {ME_train:.4f} | {ME_val:.4f} | {ME_test:.4f} | {X_LR.intercept_:7.4f} | {X_train.shape[0]:5d} | {X_val.shape[0]:5d} | {X_test.shape[0]:^4d} | {X_train.shape[1]:^5d} | {np.sum(np.abs(X_LR.coef_) > 0):^5d} |')\n",
    "    print(\"Parameters \",value, l1_ratio )\n",
    "    print(f'Mean | {np.mean(MSEs_train):.4f} | {np.mean(MSEs_val):.4f} | {np.mean(MSEs_test):.4f} | {np.mean(MEs_train):.4f} | {np.mean(MEs_val):.4f} | {np.mean(MEs_test):.4f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust l1_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |    Mean squared error    |        Mean  error       |         |       # samples      | #     | # NZ \n",
      "Fold |--------+--------+--------|--------+--------+--------| Inter-  |-------+-------+------| feat- | feat-\n",
      "     | Train  |   Val  |  Test  | Train  |   Val  |  Test  |  cept   | Train |  Val  | Test | ures  | ures \n",
      "-----+--------+--------+--------+--------+--------+--------+---------+-------+-------+------+-------+------\n",
      "Parameters  0.2 0.1\n",
      "Mean | 0.1264 | 0.1310 | 0.0403 | 0.2922 | 0.2975 | 0.1615 |\n",
      "Parameters  0.2 0.2\n",
      "Mean | 0.1285 | 0.1319 | 0.0361 | 0.2951 | 0.2989 | 0.1509 |\n",
      "Parameters  0.2 0.3\n",
      "Mean | 0.1296 | 0.1325 | 0.0346 | 0.2965 | 0.2998 | 0.1478 |\n",
      "Parameters  0.2 0.4\n",
      "Mean | 0.1302 | 0.1328 | 0.0332 | 0.2976 | 0.3002 | 0.1448 |\n",
      "Parameters  0.2 0.5\n",
      "Mean | 0.1308 | 0.1329 | 0.0323 | 0.2983 | 0.3006 | 0.1433 |\n",
      "Parameters  0.2 0.6\n",
      "Mean | 0.1311 | 0.1331 | 0.0319 | 0.2989 | 0.3010 | 0.1433 |\n",
      "Parameters  0.2 0.7\n",
      "Mean | 0.1314 | 0.1333 | 0.0318 | 0.2994 | 0.3013 | 0.1438 |\n",
      "Parameters  0.2 0.8\n",
      "Mean | 0.1316 | 0.1334 | 0.0318 | 0.2998 | 0.3016 | 0.1446 |\n",
      "Parameters  0.2 0.9\n",
      "Mean | 0.1318 | 0.1335 | 0.0319 | 0.3002 | 0.3019 | 0.1459 |\n",
      "Parameters  0.2 1\n",
      "Mean | 0.1321 | 0.1337 | 0.0322 | 0.3006 | 0.3022 | 0.1472 |\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "# Suppress ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "# Define your ElasticNet model with specific hyperparameters\n",
    "alpha = 0.2\n",
    "l1_ratio = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1] \n",
    "print('     |    Mean squared error    |        Mean  error       |         |       # samples      | #     | # NZ ')\n",
    "print('Fold |--------+--------+--------|--------+--------+--------| Inter-  |-------+-------+------| feat- | feat-')\n",
    "print('     | Train  |   Val  |  Test  | Train  |   Val  |  Test  |  cept   | Train |  Val  | Test | ures  | ures ')\n",
    "print('-----+--------+--------+--------+--------+--------+--------+---------+-------+-------+------+-------+------')\n",
    "\n",
    "for value in l1_ratio:\n",
    "\n",
    "    model = ElasticNet(alpha=alpha, l1_ratio=value, selection=\"random\")\n",
    "    #print(f'Investigating performance of {model} model...')\n",
    "\n",
    "    MSEs_train = []\n",
    "    MSEs_val = []\n",
    "    MSEs_test = []\n",
    "    MEs_train = []\n",
    "    MEs_val = []\n",
    "    MEs_test = []\n",
    "\n",
    "    \n",
    "    for val_fold in [1,2,3,4,5]:\n",
    "\n",
    "        # Extract dataframes\n",
    "        df_train = responses_ARAUS[(responses_ARAUS['info.fold'] != val_fold) & (responses_ARAUS['info.fold'] > 0)] # For the training set, use all samples that are not in the test set (fold 0) and current validation fold.\n",
    "        df_val   = responses_ARAUS[responses_ARAUS['info.fold'] == val_fold]\n",
    "        df_test  = responses_ARAUS[responses_ARAUS['info.fold'] == 0].groupby(['info.soundscape','info.masker','info.smr']).mean() # For the test set, the same 48 stimuli were shown to all participants so we take the mean of their ratings as the ground truth\n",
    "\n",
    "        # Get ground-truth labels\n",
    "        Y_train = df_train['info.E_ground_truth'].values\n",
    "        Y_val = df_val['info.E_ground_truth'].values\n",
    "        Y_test = df_test['info.E_ground_truth'].values\n",
    "\n",
    "        # Get features normalized_data = (data - min) / (max-min)\n",
    "        X_train = df_train[ARAUS_features+masker_features].values\n",
    "        X_val =df_val[ARAUS_features+masker_features].values\n",
    "        X_test = df_test[ARAUS_features+masker_features].values\n",
    "        \"\"\" X_train, min, max = normalize_columns_minmax(df_train[ARAUS_features].values)\n",
    "        X_val =((df_val[ARAUS_features].values)- min) / (max-min)\n",
    "        X_test = ((df_test[ARAUS_features].values)- min) / (max-min) \"\"\"\n",
    "        \"\"\" scaler.fit(df_train[ARAUS_features].values)\n",
    "\n",
    "        # Transform the data using the scaler\n",
    "        X_train = scaler.transform(df_train[ARAUS_features].values)\n",
    "        X_val =scaler.transform(df_val[ARAUS_features].values)\n",
    "        X_test = scaler.transform(df_test[ARAUS_features].values) \"\"\"\n",
    "\n",
    "        # Fit model\n",
    "        X_LR = model.fit(X_train, Y_train)\n",
    "\n",
    "        # Get MSEs\n",
    "        MSE_train = np.mean((clip(X_LR.predict(X_train)) - Y_train)**2)\n",
    "        MSE_val = np.mean((clip(X_LR.predict(X_val)) - Y_val)**2)\n",
    "        MSE_test = np.mean((clip(X_LR.predict(X_test)) - Y_test)**2)\n",
    "        ME_train = np.mean(np.abs(clip(X_LR.predict(X_train)) - Y_train))\n",
    "        ME_val = np.mean(np.abs(clip(X_LR.predict(X_val)) - Y_val))\n",
    "        ME_test = np.mean(np.abs(clip(X_LR.predict(X_test)) - Y_test))\n",
    "\n",
    "        # Add metrics\n",
    "        MSEs_train.append(MSE_train)\n",
    "        MSEs_val.append(MSE_val)\n",
    "        MSEs_test.append(MSE_test)\n",
    "        MEs_train.append(ME_train)\n",
    "        MEs_val.append(ME_val)\n",
    "        MEs_test.append(ME_test)\n",
    "\n",
    "        #print(f'{val_fold:4d} | {MSE_train:.4f} | {MSE_val:.4f} | {MSE_test:.4f} | {ME_train:.4f} | {ME_val:.4f} | {ME_test:.4f} | {X_LR.intercept_:7.4f} | {X_train.shape[0]:5d} | {X_val.shape[0]:5d} | {X_test.shape[0]:^4d} | {X_train.shape[1]:^5d} | {np.sum(np.abs(X_LR.coef_) > 0):^5d} |')\n",
    "    print(\"Parameters \",alpha, value )\n",
    "    print(f'Mean | {np.mean(MSEs_train):.4f} | {np.mean(MSEs_val):.4f} | {np.mean(MSEs_test):.4f} | {np.mean(MEs_train):.4f} | {np.mean(MEs_val):.4f} | {np.mean(MEs_test):.4f} |')\n",
    "    \"\"\" coefficients = X_LR.coef_\n",
    "    feature_importances = list(zip(ARAUS_features+masker_features, coefficients))\n",
    "    feature_importances.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    list_features=[]\n",
    "    for feature, importance in feature_importances:\n",
    "        if float(importance)!=0:\n",
    "            print(f\"{feature}: {importance}\")\n",
    "            list_features.append(feature)\n",
    "    print(\"LIST \",list_features) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |    Mean squared error    |        Mean  error       |         |       # samples      | #     | # NZ \n",
      "Fold |--------+--------+--------|--------+--------+--------| Inter-  |-------+-------+------| feat- | feat-\n",
      "     | Train  |   Val  |  Test  | Train  |   Val  |  Test  |  cept   | Train |  Val  | Test | ures  | ures \n",
      "-----+--------+--------+--------+--------+--------+--------+---------+-------+-------+------+-------+------\n",
      "Parameters  0.2 0.5\n",
      "Mean | 0.1308 | 0.1329 | 0.0323 | 0.2983 | 0.3006 | 0.1433 |\n",
      "ARAUS.LA.max: 0.008389068033380282\n",
      "ARAUS.energy_frequency.00630_0: 0.0046823517018782885\n",
      "ARAUS.energy_frequency.00500_0: 0.0035934993776887453\n",
      "ARAUS.loudness.max: 0.0015935251646821515\n",
      "ARAUS.LA.p10: 0.0015271507806383247\n",
      "ARAUS.LC.max: 0.0006861040617556041\n",
      "ARAUS.energy_frequency.20000_0: -0.0006769021971379923\n",
      "LIST  ['ARAUS.LA.max', 'ARAUS.energy_frequency.00630_0', 'ARAUS.energy_frequency.00500_0', 'ARAUS.loudness.max', 'ARAUS.LA.p10', 'ARAUS.LC.max', 'ARAUS.energy_frequency.20000_0']\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "# Suppress ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "# Define your ElasticNet model with specific hyperparameters\n",
    "alpha = 0.2\n",
    "l1_ratio = 0.5\n",
    "print('     |    Mean squared error    |        Mean  error       |         |       # samples      | #     | # NZ ')\n",
    "print('Fold |--------+--------+--------|--------+--------+--------| Inter-  |-------+-------+------| feat- | feat-')\n",
    "print('     | Train  |   Val  |  Test  | Train  |   Val  |  Test  |  cept   | Train |  Val  | Test | ures  | ures ')\n",
    "print('-----+--------+--------+--------+--------+--------+--------+---------+-------+-------+------+-------+------')\n",
    "\n",
    "\n",
    "\n",
    "model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, selection=\"random\")\n",
    "#print(f'Investigating performance of {model} model...')\n",
    "\n",
    "MSEs_train = []\n",
    "MSEs_val = []\n",
    "MSEs_test = []\n",
    "MEs_train = []\n",
    "MEs_val = []\n",
    "MEs_test = []\n",
    "\n",
    "\n",
    "for val_fold in [1,2,3,4,5]:\n",
    "\n",
    "    # Extract dataframes\n",
    "    df_train = responses_ARAUS[(responses_ARAUS['info.fold'] != val_fold) & (responses_ARAUS['info.fold'] > 0)] # For the training set, use all samples that are not in the test set (fold 0) and current validation fold.\n",
    "    df_val   = responses_ARAUS[responses_ARAUS['info.fold'] == val_fold]\n",
    "    df_test  = responses_ARAUS[responses_ARAUS['info.fold'] == 0].groupby(['info.soundscape','info.masker','info.smr']).mean() # For the test set, the same 48 stimuli were shown to all participants so we take the mean of their ratings as the ground truth\n",
    "\n",
    "    # Get ground-truth labels\n",
    "    Y_train = df_train['info.E_ground_truth'].values\n",
    "    Y_val = df_val['info.E_ground_truth'].values\n",
    "    Y_test = df_test['info.E_ground_truth'].values\n",
    "\n",
    "    # Get features normalized_data = (data - min) / (max-min)\n",
    "    X_train = df_train[ARAUS_features+masker_features].values\n",
    "    X_val =df_val[ARAUS_features+masker_features].values\n",
    "    X_test = df_test[ARAUS_features+masker_features].values\n",
    "    \"\"\" X_train, min, max = normalize_columns_minmax(df_train[ARAUS_features].values)\n",
    "    X_val =((df_val[ARAUS_features].values)- min) / (max-min)\n",
    "    X_test = ((df_test[ARAUS_features].values)- min) / (max-min) \"\"\"\n",
    "    \"\"\" scaler.fit(df_train[ARAUS_features].values)\n",
    "\n",
    "    # Transform the data using the scaler\n",
    "    X_train = scaler.transform(df_train[ARAUS_features].values)\n",
    "    X_val =scaler.transform(df_val[ARAUS_features].values)\n",
    "    X_test = scaler.transform(df_test[ARAUS_features].values) \"\"\"\n",
    "\n",
    "    # Fit model\n",
    "    X_LR = model.fit(X_train, Y_train)\n",
    "\n",
    "    # Get MSEs\n",
    "    MSE_train = np.mean((clip(X_LR.predict(X_train)) - Y_train)**2)\n",
    "    MSE_val = np.mean((clip(X_LR.predict(X_val)) - Y_val)**2)\n",
    "    MSE_test = np.mean((clip(X_LR.predict(X_test)) - Y_test)**2)\n",
    "    ME_train = np.mean(np.abs(clip(X_LR.predict(X_train)) - Y_train))\n",
    "    ME_val = np.mean(np.abs(clip(X_LR.predict(X_val)) - Y_val))\n",
    "    ME_test = np.mean(np.abs(clip(X_LR.predict(X_test)) - Y_test))\n",
    "\n",
    "    # Add metrics\n",
    "    MSEs_train.append(MSE_train)\n",
    "    MSEs_val.append(MSE_val)\n",
    "    MSEs_test.append(MSE_test)\n",
    "    MEs_train.append(ME_train)\n",
    "    MEs_val.append(ME_val)\n",
    "    MEs_test.append(ME_test)\n",
    "\n",
    "    #print(f'{val_fold:4d} | {MSE_train:.4f} | {MSE_val:.4f} | {MSE_test:.4f} | {ME_train:.4f} | {ME_val:.4f} | {ME_test:.4f} | {X_LR.intercept_:7.4f} | {X_train.shape[0]:5d} | {X_val.shape[0]:5d} | {X_test.shape[0]:^4d} | {X_train.shape[1]:^5d} | {np.sum(np.abs(X_LR.coef_) > 0):^5d} |')\n",
    "print(\"Parameters \",alpha, l1_ratio )\n",
    "print(f'Mean | {np.mean(MSEs_train):.4f} | {np.mean(MSEs_val):.4f} | {np.mean(MSEs_test):.4f} | {np.mean(MEs_train):.4f} | {np.mean(MEs_val):.4f} | {np.mean(MEs_test):.4f} |')\n",
    "coefficients = X_LR.coef_\n",
    "feature_importances = list(zip(ARAUS_features+masker_features, coefficients))\n",
    "feature_importances.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "list_features=[]\n",
    "for feature, importance in feature_importances:\n",
    "    if float(importance)!=0:\n",
    "        print(f\"{feature}: {importance}\")\n",
    "        list_features.append(feature)\n",
    "print(\"LIST \",list_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
