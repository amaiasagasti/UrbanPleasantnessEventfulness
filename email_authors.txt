I am Amaia Sagasti, a Music and Sound Computing student in Pompeu Fabra University (Barcelona). I am developping my Master thesis on the assessment of urban sound environments, supervised by Frederic Font and Martin Rocamora (both on copy). I am adressing to you due to some doubts I have regarding the feature generation of ARAUS dataset.

My project consists on training ML/DL models, for which I am using the ARAUS dataset, and then testing its performance on our own recordings. However, for this last part, I need to be able to replicate the way you generate the acoustic and psychoacoustic annotations, so that later on I can extract the same features (making sure I am following the correct procedure) on new soundscape audios.

It is mentioned in your publication "ARAUS: A Large-Scale Dataset and Baseline Models of Affective Responses to Augmented Urban Soundscapes" that you used Artemis Suite from Head Acoustics, however, you suggest to use Mosqito, the python library, as an open source approach (which is mine). Even though the origin of the features is explained in both the paper and the README file of the ARAUS-baseline-code, I am struggling to get some of the features, and the ones I am getting are not exact. 
For now, I am using Mosqito's and other fuctions that supposedly calculate the desired features according to the specified standards to generate the features of the ARAUS augmented soundscapes. Then I can compare the values I obtain to the "ground truth" values. This way, I can check if my functions are correct. As I have mentioned, I have achieved some acceptable results for some features (sharpness, LAeq, LCeq) but for some others I am struggling a bit more (for loudness, roughness, fluctuation strength and M####_# I can see that the tendency looks correct even though the values are far from the ground-truth.
In the following points I am exposing the different difficulties I am encountering:

- Firstly, in order to correctly load the audios and do the calculations, I need the wav calibration gain from the digital signal to the pressure signal, in Pascals. The file results.csv does not provide
such information. However, it does provide Leq, so my approach has been to calculate Leq on the signal without calibration, then calculate the difference between the obtained value and the
Leq provided, and the result (in linear) is the gain I am applying. I have tested these calculated gains on some audios calculating the features, and results seem to be okay for some features( they are not perfect, it is hard to tell if the error is found on the calculated gain or on the functions I am using to calculate the features... but i guess that if LAeq values look correct, they should be pretty accurate). Were the Leq values included for this purpose? Or what other approach is there to get this gain or to do the transformation?

- Tonality calculation (ECMA-74) is not found in Mosqito's library. Do you know anywhere I could find a function that calculates it according to the standard? I could create the function myself but I'm afraid this approach takes too much time as it is not the main scope of my project. MENTION EFFECT?

- Regarding the calculation of features, the M####_# feature does not come from any standard, but you provide a short explanation on how to calculate it. I have taken the following steps in order to calculate them and I have not obtained good results (SHOW GRAPHS, the shape looks similar on mid-high frequencies but values are far from good).
    1) For each audio of 30 seconds, I load the right channel with calculated wav calibration gain.
    2) I carry out the analysis using hanning window of 8192 points, with an overlap of 4096 samples
    3) To each temporal window of 8192:
            1) I apply an FFT and multiply by 1/8192 to normalize the magnitude spectrum
            2) I obtain an spectrum of 8192 points, but I remove the first half, keeping only the positive frequencies part (4096 points)
            3) I have two vectors of 4096 points (one containing the magnitude spectrum values and another one containing the frequency axis values)
            4) I calculate the frequency limits for each third-octave band as indicated
            5) Using the 4096 samples frequency vector, for each band, I select which samples are contained in the band, and sum their corresponding magnitude values ^2 (power). 
            6) I end up with a vector of size = number of bands, containing the sum of the powers (per band, still in linear)     4) Now i have an array of size number_windows*number_bands, so I do the mean over time (over the time windows) of the power per band. I end up with a vector of size number_bands, that I transform to log with 20log(vector/0.00002). Each position of this vector corresponds to a M####_# feature.Is there something wrong with this procedure? There are parts of it that were not specified and I just carried out from intuition... Any help or further explanation could be appreciated!
- Regarding roughness and fluctuation, I am following what is specified in the Zwicker's book, but I am not sure I am using it correctly, I am using N_spec (specific loudness) for its computation. How is it calculated in your procedure? As you can see in the graphs, The values are not accurate, even though the tendency looks similar... GRAPHS
Thank you very much for the time you took to read this email. Of course, I do not expect you to answer every single point in detail (the work is still in progress and I am still working on them trying to get the most accurate values), but any tip or comment that you may have is very welcomed and will be of much help. 
Again, thank you very much.
Amaia Sagasti


wooi002@ntu.edu.sg 
ztong@ntu.edu.sg 
bhanlam@ntu.edu.sg 
ewsgan@ntu.edu.sg 

kwatcharasupat@gatech.edu

jyhong@cnu.ac.kr 
